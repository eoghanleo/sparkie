-- ================================================================================
-- Sparkie RAG Evaluation using Snowflake AI_COMPLETE (Latest Cortex Function)
-- ================================================================================
--
-- Uses the latest Snowflake Cortex AI function: AI_COMPLETE
-- Model: llama4-maverick (same as your Python evaluation)
-- All data stays within Snowflake - no external API calls needed
--
-- TWO-PROMPT EVALUATION SYSTEM:
-- 1. Context-Heavy Evaluation: Hallucination + Citation Accuracy
-- 2. Question-Answer Evaluation: Relevance + Completeness
--
-- ================================================================================

USE DATABASE SPARKIE_V2_DB;
USE SCHEMA ELECTRICAL_STANDARDS;
USE WAREHOUSE ELECTRICAL_RAG_WH;

-- ================================================================================
-- STEP 1: Select interactions that need evaluation
-- ================================================================================

CREATE OR REPLACE TEMPORARY TABLE interactions_to_evaluate AS
SELECT
    i.interaction_id,
    i.user_query AS question,
    i.answer_text AS answer,
    i.metadata AS interaction_metadata
FROM RAG_INTERACTION i
LEFT JOIN EVAL_RESULTS e ON i.interaction_id = e.interaction_id
WHERE e.eval_id IS NULL  -- Only unevaluated interactions
  AND i.status = 'success'
  AND i.answer_text IS NOT NULL
  AND i.ts >= DATEADD(day, -7, CURRENT_TIMESTAMP())  -- Last 7 days
LIMIT 10;  -- Batch size - adjust as needed

-- Verify data
SELECT COUNT(*) AS interactions_to_eval FROM interactions_to_evaluate;

-- ================================================================================
-- STEP 2: Build context from chunks metadata
-- ================================================================================

CREATE OR REPLACE TEMPORARY TABLE context_prepared AS
SELECT
    i.interaction_id,
    i.question,
    i.answer,
    -- Extract and aggregate text chunks with proper ordering
    LISTAGG(
        '[' || chunk.seq || '] ' ||
        '(Page ' || COALESCE(chunk.value:page_number::STRING, 'unknown') || ', ' ||
        COALESCE(chunk.value:content_type::STRING, 'text') || '):\n' ||
        COALESCE(
            chunk.value:text_content::STRING,
            chunk.value:chunk_preview::STRING,
            chunk.value:description::STRING,
            ''
        ),
        '\n\n'
    ) WITHIN GROUP (ORDER BY chunk.value:similarity::FLOAT DESC NULLS LAST) AS context_text,
    -- Extract clause references from answer using regex
    ARRAY_TO_STRING(
        ARRAY_DISTINCT(
            FLATTEN(
                REGEXP_SUBSTR_ALL(i.answer, 'Clause\\s+([\\d.]+)', 1, 1, 'ie', 1)
            )
        ),
        ', '
    ) AS cited_clauses_str
FROM interactions_to_evaluate i,
LATERAL FLATTEN(input => i.interaction_metadata:chunks_metadata, outer => true) chunk
GROUP BY i.interaction_id, i.question, i.answer;

-- Verify context was built
SELECT
    interaction_id,
    LENGTH(context_text) AS context_length,
    cited_clauses_str
FROM context_prepared
LIMIT 3;

-- ================================================================================
-- STEP 3: Prompt 1 - Context-Heavy Evaluation (Hallucination + Citation)
-- ================================================================================

CREATE OR REPLACE TEMPORARY TABLE eval_results_1 AS
SELECT
    interaction_id,
    question,
    answer,
    context_text,
    cited_clauses_str,
    -- Call AI_COMPLETE with prompt object
    SNOWFLAKE.CORTEX.AI_COMPLETE(
        'llama4-maverick',
        SNOWFLAKE.CORTEX.PROMPT(
            'You are an expert evaluator for a RAG system that provides AS3000 electrical standards information.

Your task is to evaluate TWO dimensions of the generated answer:
1. HALLUCINATION DETECTION - Are all claims supported by the retrieved context?
2. CITATION ACCURACY - Are AS3000 clause references correct?

═══════════════════════════════════════════════════════════════

QUESTION (from user):
{0}

ANSWER (generated by RAG system):
{1}

RETRIEVED CONTEXT (from AS3000 standards):
{2}

CITED CLAUSES (extracted from answer): {3}

═══════════════════════════════════════════════════════════════

TASK 1 - HALLUCINATION DETECTION:

1. Extract all factual claims from the ANSWER (measurements, requirements, clause references, safety rules)
2. For each claim, verify if it appears in or is directly inferable from the RETRIEVED CONTEXT
3. List any claims that are NOT supported by the context
4. Provide a hallucination score from 0.0 to 1.0 using precise decimals:
   - 0.0-0.15: All claims fully supported
   - 0.16-0.35: Minor unsupported details
   - 0.36-0.65: Moderate hallucinations
   - 0.66-0.89: Significant fabricated information
   - 0.90-1.0: Severe hallucinations

TASK 2 - CITATION ACCURACY:

1. Verify if the cited clauses are accurate based on the retrieved context
2. Assess if the citations are relevant to the question
3. Provide a citation accuracy score from 0.0 to 1.0 using precise decimals:
   - 0.90-1.0: All citations accurate
   - 0.70-0.89: Citations mostly accurate
   - 0.35-0.69: Some citations accurate, some incorrect
   - 0.10-0.34: Most citations incorrect or irrelevant
   - 0.0-0.09: No citations or all wrong

═══════════════════════════════════════════════════════════════

OUTPUT FORMAT (JSON only, no other text):
{
    "hallucination_score": 0.0,
    "unsupported_claims": ["claim 1", "claim 2"],
    "hallucination_reasoning": "Detailed explanation",
    "citation_accuracy": 0.0,
    "citation_reasoning": "Detailed explanation"
}

Respond with ONLY the JSON object.',
            question,
            answer,
            COALESCE(context_text, 'No context available'),
            COALESCE(cited_clauses_str, 'None')
        ),
        {
            'temperature': 0.0,
            'max_tokens': 1400,
            'top_p': 1.0
        }
    ) AS llm_response_1
FROM context_prepared;

-- Parse JSON response from Prompt 1
CREATE OR REPLACE TEMPORARY TABLE eval_1_parsed AS
SELECT
    interaction_id,
    question,
    answer,
    llm_response_1,
    TRY_PARSE_JSON(llm_response_1) AS response_json_1,
    -- Extract scores
    COALESCE(response_json_1:hallucination_score::FLOAT, NULL) AS hallucination_score,
    COALESCE(response_json_1:citation_accuracy::FLOAT, NULL) AS citation_accuracy,
    -- Extract reasoning
    COALESCE(response_json_1:hallucination_reasoning::STRING, '') AS hallucination_details,
    COALESCE(response_json_1:citation_reasoning::STRING, '') AS citation_details,
    -- Extract unsupported claims
    COALESCE(response_json_1:unsupported_claims, ARRAY_CONSTRUCT()) AS unsupported_claims
FROM eval_results_1;

-- Verify Prompt 1 results
SELECT
    interaction_id,
    hallucination_score,
    citation_accuracy,
    CASE
        WHEN hallucination_score IS NULL THEN 'PARSE_ERROR'
        WHEN citation_accuracy IS NULL THEN 'PARSE_ERROR'
        ELSE 'SUCCESS'
    END AS status
FROM eval_1_parsed;

-- ================================================================================
-- STEP 4: Prompt 2 - Question-Answer Evaluation (Relevance + Completeness)
-- ================================================================================

CREATE OR REPLACE TEMPORARY TABLE eval_results_2 AS
SELECT
    e1.interaction_id,
    e1.question,
    e1.answer,
    -- Call AI_COMPLETE with prompt object
    SNOWFLAKE.CORTEX.AI_COMPLETE(
        'llama4-maverick',
        SNOWFLAKE.CORTEX.PROMPT(
            'You are an expert evaluator for a RAG system that provides AS3000 electrical standards information.

Your task is to evaluate TWO dimensions of the generated answer:
1. RELEVANCE - Does the answer address the question?
2. COMPLETENESS - Are all parts of the question covered?

═══════════════════════════════════════════════════════════════

QUESTION (from user):
{0}

ANSWER (generated by RAG system):
{1}

═══════════════════════════════════════════════════════════════

TASK 1 - RELEVANCE:

Rate the relevance of the ANSWER to the QUESTION on a scale from 0.0 to 1.0 using precise decimals:
- 0.95-1.0: Directly and completely addresses the question
- 0.75-0.94: Mostly relevant with minor gaps
- 0.40-0.74: Partially relevant with significant gaps
- 0.15-0.39: Tangentially related
- 0.0-0.14: Completely irrelevant

TASK 2 - COMPLETENESS:

Rate the completeness of the ANSWER on a scale from 0.0 to 1.0 using precise decimals:
- 0.92-1.0: All parts thoroughly addressed
- 0.70-0.91: Main points covered with minor details missing
- 0.35-0.69: Partially complete with significant gaps
- 0.10-0.34: Minimal coverage
- 0.0-0.09: Question not addressed

═══════════════════════════════════════════════════════════════

OUTPUT FORMAT (JSON only, no other text):
{
    "answer_relevance": 0.0,
    "relevance_reasoning": "Detailed explanation",
    "answer_completeness": 0.0,
    "completeness_reasoning": "Detailed explanation"
}

Respond with ONLY the JSON object.',
            e1.question,
            e1.answer
        ),
        {
            'temperature': 0.0,
            'max_tokens': 1000,
            'top_p': 1.0
        }
    ) AS llm_response_2
FROM eval_1_parsed e1;

-- Parse JSON response from Prompt 2
CREATE OR REPLACE TEMPORARY TABLE eval_2_parsed AS
SELECT
    interaction_id,
    llm_response_2,
    TRY_PARSE_JSON(llm_response_2) AS response_json_2,
    -- Extract scores
    COALESCE(response_json_2:answer_relevance::FLOAT, NULL) AS answer_relevance,
    COALESCE(response_json_2:answer_completeness::FLOAT, NULL) AS answer_completeness,
    -- Extract reasoning
    COALESCE(response_json_2:relevance_reasoning::STRING, '') AS relevance_details,
    COALESCE(response_json_2:completeness_reasoning::STRING, '') AS completeness_details
FROM eval_results_2;

-- Verify Prompt 2 results
SELECT
    interaction_id,
    answer_relevance,
    answer_completeness,
    CASE
        WHEN answer_relevance IS NULL THEN 'PARSE_ERROR'
        WHEN answer_completeness IS NULL THEN 'PARSE_ERROR'
        ELSE 'SUCCESS'
    END AS status
FROM eval_2_parsed;

-- ================================================================================
-- STEP 5: Combine results from both prompts
-- ================================================================================

CREATE OR REPLACE TEMPORARY TABLE combined_eval_results AS
SELECT
    e1.interaction_id,
    e1.hallucination_score,
    e1.citation_accuracy,
    e1.hallucination_details,
    e1.citation_details,
    e1.unsupported_claims,
    e2.answer_relevance,
    e2.answer_completeness,
    e2.relevance_details,
    e2.completeness_details
FROM eval_1_parsed e1
JOIN eval_2_parsed e2 ON e1.interaction_id = e2.interaction_id;

-- View combined results before inserting
SELECT
    interaction_id,
    ROUND(hallucination_score, 3) AS hallucination,
    ROUND(citation_accuracy, 3) AS citation,
    ROUND(answer_relevance, 3) AS relevance,
    ROUND(answer_completeness, 3) AS completeness,
    CASE
        WHEN hallucination_score IS NULL OR citation_accuracy IS NULL OR
             answer_relevance IS NULL OR answer_completeness IS NULL
        THEN 'INCOMPLETE'
        ELSE 'SUCCESS'
    END AS eval_status
FROM combined_eval_results;

-- ================================================================================
-- STEP 6: Insert into EVAL_RESULTS table
-- ================================================================================

INSERT INTO EVAL_RESULTS (
    eval_id,
    interaction_id,
    eval_run_id,
    answer_relevance,
    answer_completeness,
    hallucination_score,
    citation_accuracy,
    correct_clause_ref,
    cited_clauses,
    expected_clause,
    technical_accuracy,
    judge_model,
    judge_prompt_tokens,
    judge_response_tokens,
    judge_reasoning,
    eval_method,
    eval_version,
    metadata,
    evaluated_at
)
SELECT
    UUID_STRING() AS eval_id,
    interaction_id,
    NULL AS eval_run_id,
    answer_relevance,
    answer_completeness,
    hallucination_score,
    citation_accuracy,
    NULL AS correct_clause_ref,
    NULL AS cited_clauses,
    NULL AS expected_clause,
    NULL AS technical_accuracy,
    'llama4-maverick' AS judge_model,
    NULL AS judge_prompt_tokens,
    NULL AS judge_response_tokens,
    'Hallucination: ' || hallucination_details || ' | ' ||
    'Citations: ' || citation_details || ' | ' ||
    'Relevance: ' || relevance_details || ' | ' ||
    'Completeness: ' || completeness_details AS judge_reasoning,
    'snowflake_ai_complete' AS eval_method,
    '2.0-ai-complete' AS eval_version,
    OBJECT_CONSTRUCT(
        'hallucination_details', hallucination_details,
        'citation_details', citation_details,
        'relevance_details', relevance_details,
        'completeness_details', completeness_details,
        'unsupported_claims', unsupported_claims
    ) AS metadata,
    CURRENT_TIMESTAMP() AS evaluated_at
FROM combined_eval_results
WHERE hallucination_score IS NOT NULL  -- Only insert successful evaluations
  AND citation_accuracy IS NOT NULL
  AND answer_relevance IS NOT NULL
  AND answer_completeness IS NOT NULL;

-- ================================================================================
-- STEP 7: View inserted results
-- ================================================================================

SELECT
    interaction_id,
    ROUND(hallucination_score, 3) AS hallucination,
    ROUND(citation_accuracy, 3) AS citation,
    ROUND(answer_relevance, 3) AS relevance,
    ROUND(answer_completeness, 3) AS completeness,
    eval_method,
    eval_version,
    judge_model,
    evaluated_at
FROM EVAL_RESULTS
WHERE eval_method = 'snowflake_ai_complete'
ORDER BY evaluated_at DESC
LIMIT 10;

-- ================================================================================
-- STEP 8: Cleanup temporary tables
-- ================================================================================

DROP TABLE IF EXISTS interactions_to_evaluate;
DROP TABLE IF EXISTS context_prepared;
DROP TABLE IF EXISTS eval_results_1;
DROP TABLE IF EXISTS eval_1_parsed;
DROP TABLE IF EXISTS eval_results_2;
DROP TABLE IF EXISTS eval_2_parsed;
DROP TABLE IF EXISTS combined_eval_results;

-- ================================================================================
-- MONITORING QUERIES
-- ================================================================================

-- Check evaluation coverage
SELECT
    COUNT(DISTINCT i.interaction_id) AS total_interactions,
    COUNT(DISTINCT e.eval_id) AS evaluated_interactions,
    ROUND(COUNT(DISTINCT e.eval_id) * 100.0 / NULLIF(COUNT(DISTINCT i.interaction_id), 0), 2) AS coverage_pct
FROM RAG_INTERACTION i
LEFT JOIN EVAL_RESULTS e ON i.interaction_id = e.interaction_id
WHERE i.status = 'success'
  AND i.ts >= DATEADD(day, -7, CURRENT_TIMESTAMP());

-- Compare evaluation methods
SELECT
    eval_method,
    judge_model,
    COUNT(*) AS eval_count,
    ROUND(AVG(hallucination_score), 3) AS avg_hallucination,
    ROUND(AVG(citation_accuracy), 3) AS avg_citation,
    ROUND(AVG(answer_relevance), 3) AS avg_relevance,
    ROUND(AVG(answer_completeness), 3) AS avg_completeness
FROM EVAL_RESULTS
WHERE evaluated_at >= DATEADD(day, -7, CURRENT_TIMESTAMP())
GROUP BY eval_method, judge_model
ORDER BY eval_count DESC;

-- ================================================================================
-- AUTOMATED SCHEDULING WITH SNOWFLAKE TASK
-- ================================================================================

/*
-- Create a task to run evaluations hourly
CREATE OR REPLACE TASK eval_new_interactions_hourly
WAREHOUSE = ELECTRICAL_RAG_WH
SCHEDULE = 'USING CRON 0 * * * * UTC'  -- Every hour
AS
BEGIN
    -- Paste the entire script here from STEP 1 through STEP 7
    -- Wrap in BEGIN...END for task execution
END;

-- Start the task
ALTER TASK eval_new_interactions_hourly RESUME;

-- Monitor task execution
SELECT *
FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(
    TASK_NAME => 'eval_new_interactions_hourly',
    SCHEDULED_TIME_RANGE_START => DATEADD(day, -1, CURRENT_TIMESTAMP())
))
ORDER BY scheduled_time DESC;

-- Suspend the task if needed
ALTER TASK eval_new_interactions_hourly SUSPEND;
*/

-- ================================================================================
-- NOTES AND BEST PRACTICES
-- ================================================================================

/*

MODELS AVAILABLE FOR AI_COMPLETE:
- llama4-maverick (recommended - matches your Python evaluation)
- llama4-scout (alternative)
- llama3.1-70b (good balance)
- llama3.1-405b (highest quality)
- claude-4-sonnet (excellent for complex reasoning)

BATCH PROCESSING:
Adjust the LIMIT in Step 1:
    LIMIT 10    -- Small batch for testing
    LIMIT 100   -- Medium batch
    LIMIT 1000  -- Large batch
    -- Remove LIMIT to process all

COST OPTIMIZATION:
- Use llama4-maverick (good balance)
- Process in smaller batches during testing
- Schedule during off-peak hours
- Monitor warehouse usage

TROUBLESHOOTING:
- If JSON parsing fails: Check llm_response_1 and llm_response_2 columns
- If no results: Verify interactions_to_evaluate has data
- If context_text is NULL: Check metadata structure in RAG_INTERACTION
- Use verification SELECT statements after each step

PROMPT OBJECT SYNTAX:
SNOWFLAKE.CORTEX.PROMPT('template with {0}, {1}', value0, value1)
- Numbered placeholders {0}, {1}, {2}, etc.
- Values can be column references or literals
- Supports text and images (TO_FILE for images)

*/
