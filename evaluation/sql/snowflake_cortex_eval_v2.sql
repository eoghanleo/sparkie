-- ================================================================================
-- Sparkie RAG Evaluation using Snowflake Cortex AI Functions (Updated)
-- ================================================================================
--
-- Uses the latest Snowflake Cortex AI SQL functions for evaluation
-- All data stays within Snowflake - no external API calls needed
--
-- CORTEX FUNCTIONS USED:
-- - SNOWFLAKE.CORTEX.COMPLETE() - LLM inference for evaluation
--
-- TWO-PROMPT EVALUATION SYSTEM:
-- 1. Context-Heavy Evaluation: Hallucination + Citation Accuracy
-- 2. Question-Answer Evaluation: Relevance + Completeness
--
-- ================================================================================

USE DATABASE SPARKIE_V2_DB;
USE SCHEMA ELECTRICAL_STANDARDS;
USE WAREHOUSE ELECTRICAL_RAG_WH;

-- ================================================================================
-- STEP 1: Select interactions that need evaluation
-- ================================================================================

CREATE OR REPLACE TEMPORARY TABLE interactions_to_evaluate AS
SELECT
    i.interaction_id,
    i.user_query AS question,
    i.answer_text AS answer,
    i.metadata AS interaction_metadata
FROM RAG_INTERACTION i
LEFT JOIN EVAL_RESULTS e ON i.interaction_id = e.interaction_id
WHERE e.eval_id IS NULL  -- Only unevaluated interactions
  AND i.status = 'success'
  AND i.answer_text IS NOT NULL
  AND i.ts >= DATEADD(day, -7, CURRENT_TIMESTAMP())  -- Last 7 days
LIMIT 10;  -- Batch size - adjust as needed

-- ================================================================================
-- STEP 2: Build context from chunks metadata
-- ================================================================================

CREATE OR REPLACE TEMPORARY TABLE context_prepared AS
SELECT
    i.interaction_id,
    i.question,
    i.answer,
    -- Extract and aggregate text chunks
    LISTAGG(
        '[' || chunk.seq || '] ' ||
        '(Page ' || COALESCE(chunk.value:page_number::STRING, 'unknown') || ', ' ||
        COALESCE(chunk.value:content_type::STRING, 'text') || '):\n' ||
        COALESCE(
            chunk.value:text_content::STRING,
            chunk.value:chunk_preview::STRING,
            chunk.value:description::STRING,
            ''
        ),
        '\n\n'
    ) WITHIN GROUP (ORDER BY chunk.value:similarity::FLOAT DESC NULLS LAST) AS context_text,
    -- Extract clause references from answer
    ARRAY_TO_STRING(
        ARRAY_DISTINCT(
            FLATTEN(
                REGEXP_SUBSTR_ALL(i.answer, 'Clause\\s+([\\d.]+)', 1, 1, 'ie', 1)
            )
        ),
        ', '
    ) AS cited_clauses_str
FROM interactions_to_evaluate i,
LATERAL FLATTEN(input => i.interaction_metadata:chunks_metadata, outer => true) chunk
GROUP BY i.interaction_id, i.question, i.answer;

-- Verify context was built
SELECT
    interaction_id,
    LENGTH(context_text) AS context_length,
    cited_clauses_str
FROM context_prepared
LIMIT 5;

-- ================================================================================
-- STEP 3: Prompt 1 - Context-Heavy Evaluation (Hallucination + Citation)
-- ================================================================================

CREATE OR REPLACE TEMPORARY TABLE eval_prompt_1 AS
SELECT
    interaction_id,
    question,
    answer,
    context_text,
    cited_clauses_str,
    -- Build evaluation prompt
    'You are an expert evaluator for a RAG system that provides AS3000 electrical standards information.

Your task is to evaluate TWO dimensions of the generated answer:
1. HALLUCINATION DETECTION - Are all claims supported by the retrieved context?
2. CITATION ACCURACY - Are AS3000 clause references correct?

═══════════════════════════════════════════════════════════════

QUESTION (from user):
' || question || '

ANSWER (generated by RAG system):
' || answer || '

RETRIEVED CONTEXT (from AS3000 standards):
' || COALESCE(context_text, 'No context available') || '

CITED CLAUSES (extracted from answer): ' || COALESCE(cited_clauses_str, 'None') || '

═══════════════════════════════════════════════════════════════

TASK 1 - HALLUCINATION DETECTION:

1. Extract all factual claims from the ANSWER (measurements, requirements, clause references, safety rules)
2. For each claim, verify if it appears in or is directly inferable from the RETRIEVED CONTEXT
3. List any claims that are NOT supported by the context
4. Provide a hallucination score from 0.0 to 1.0 using precise decimals:
   - 0.0-0.15: All claims fully supported
   - 0.16-0.35: Minor unsupported details
   - 0.36-0.65: Moderate hallucinations
   - 0.66-0.89: Significant fabricated information
   - 0.90-1.0: Severe hallucinations

TASK 2 - CITATION ACCURACY:

1. Verify if the cited clauses are accurate based on the retrieved context
2. Assess if the citations are relevant to the question
3. Provide a citation accuracy score from 0.0 to 1.0 using precise decimals:
   - 0.90-1.0: All citations accurate
   - 0.70-0.89: Citations mostly accurate
   - 0.35-0.69: Some citations accurate, some incorrect
   - 0.10-0.34: Most citations incorrect or irrelevant
   - 0.0-0.09: No citations or all wrong

═══════════════════════════════════════════════════════════════

OUTPUT FORMAT (JSON only, no other text):
{
    "hallucination_score": 0.0,
    "unsupported_claims": ["claim 1", "claim 2"],
    "hallucination_reasoning": "Detailed explanation",
    "citation_accuracy": 0.0,
    "citation_reasoning": "Detailed explanation"
}

Respond with ONLY the JSON object.' AS prompt_1
FROM context_prepared;

-- ================================================================================
-- STEP 4: Call Cortex Complete for Prompt 1
-- ================================================================================

CREATE OR REPLACE TEMPORARY TABLE eval_results_1 AS
SELECT
    interaction_id,
    question,
    answer,
    prompt_1,
    -- Call Snowflake Cortex Complete
    SNOWFLAKE.CORTEX.COMPLETE(
        'llama3.1-70b',
        prompt_1,
        {
            'temperature': 0.0,
            'max_tokens': 1400,
            'top_p': 1.0
        }
    ) AS llm_response_1,
    -- Parse JSON response
    TRY_PARSE_JSON(llm_response_1) AS response_json_1,
    -- Extract scores
    COALESCE(response_json_1:hallucination_score::FLOAT, NULL) AS hallucination_score,
    COALESCE(response_json_1:citation_accuracy::FLOAT, NULL) AS citation_accuracy,
    -- Extract reasoning
    COALESCE(response_json_1:hallucination_reasoning::STRING, '') AS hallucination_details,
    COALESCE(response_json_1:citation_reasoning::STRING, '') AS citation_details,
    -- Extract unsupported claims
    COALESCE(response_json_1:unsupported_claims, ARRAY_CONSTRUCT()) AS unsupported_claims
FROM eval_prompt_1;

-- Verify Prompt 1 results
SELECT
    interaction_id,
    hallucination_score,
    citation_accuracy,
    CASE
        WHEN hallucination_score IS NULL THEN 'PARSE_ERROR'
        ELSE 'SUCCESS'
    END AS status
FROM eval_results_1;

-- ================================================================================
-- STEP 5: Prompt 2 - Question-Answer Evaluation (Relevance + Completeness)
-- ================================================================================

CREATE OR REPLACE TEMPORARY TABLE eval_prompt_2 AS
SELECT
    r1.interaction_id,
    r1.question,
    r1.answer,
    -- Build evaluation prompt
    'You are an expert evaluator for a RAG system that provides AS3000 electrical standards information.

Your task is to evaluate TWO dimensions of the generated answer:
1. RELEVANCE - Does the answer address the question?
2. COMPLETENESS - Are all parts of the question covered?

═══════════════════════════════════════════════════════════════

QUESTION (from user):
' || r1.question || '

ANSWER (generated by RAG system):
' || r1.answer || '

═══════════════════════════════════════════════════════════════

TASK 1 - RELEVANCE:

Rate the relevance of the ANSWER to the QUESTION on a scale from 0.0 to 1.0 using precise decimals:
- 0.95-1.0: Directly and completely addresses the question
- 0.75-0.94: Mostly relevant with minor gaps
- 0.40-0.74: Partially relevant with significant gaps
- 0.15-0.39: Tangentially related
- 0.0-0.14: Completely irrelevant

TASK 2 - COMPLETENESS:

Rate the completeness of the ANSWER on a scale from 0.0 to 1.0 using precise decimals:
- 0.92-1.0: All parts thoroughly addressed
- 0.70-0.91: Main points covered with minor details missing
- 0.35-0.69: Partially complete with significant gaps
- 0.10-0.34: Minimal coverage
- 0.0-0.09: Question not addressed

═══════════════════════════════════════════════════════════════

OUTPUT FORMAT (JSON only, no other text):
{
    "answer_relevance": 0.0,
    "relevance_reasoning": "Detailed explanation",
    "answer_completeness": 0.0,
    "completeness_reasoning": "Detailed explanation"
}

Respond with ONLY the JSON object.' AS prompt_2
FROM eval_results_1 r1;

-- ================================================================================
-- STEP 6: Call Cortex Complete for Prompt 2
-- ================================================================================

CREATE OR REPLACE TEMPORARY TABLE eval_results_2 AS
SELECT
    p2.interaction_id,
    p2.question,
    p2.answer,
    -- Call Snowflake Cortex Complete
    SNOWFLAKE.CORTEX.COMPLETE(
        'llama3.1-70b',
        p2.prompt_2,
        {
            'temperature': 0.0,
            'max_tokens': 1000,
            'top_p': 1.0
        }
    ) AS llm_response_2,
    -- Parse JSON response
    TRY_PARSE_JSON(llm_response_2) AS response_json_2,
    -- Extract scores
    COALESCE(response_json_2:answer_relevance::FLOAT, NULL) AS answer_relevance,
    COALESCE(response_json_2:answer_completeness::FLOAT, NULL) AS answer_completeness,
    -- Extract reasoning
    COALESCE(response_json_2:relevance_reasoning::STRING, '') AS relevance_details,
    COALESCE(response_json_2:completeness_reasoning::STRING, '') AS completeness_details
FROM eval_prompt_2 p2;

-- Verify Prompt 2 results
SELECT
    interaction_id,
    answer_relevance,
    answer_completeness,
    CASE
        WHEN answer_relevance IS NULL THEN 'PARSE_ERROR'
        ELSE 'SUCCESS'
    END AS status
FROM eval_results_2;

-- ================================================================================
-- STEP 7: Combine results from both prompts
-- ================================================================================

CREATE OR REPLACE TEMPORARY TABLE combined_eval_results AS
SELECT
    r1.interaction_id,
    r1.hallucination_score,
    r1.citation_accuracy,
    r1.hallucination_details,
    r1.citation_details,
    r1.unsupported_claims,
    r2.answer_relevance,
    r2.answer_completeness,
    r2.relevance_details,
    r2.completeness_details
FROM eval_results_1 r1
JOIN eval_results_2 r2 ON r1.interaction_id = r2.interaction_id;

-- View combined results before inserting
SELECT
    interaction_id,
    ROUND(hallucination_score, 3) AS hallucination,
    ROUND(citation_accuracy, 3) AS citation,
    ROUND(answer_relevance, 3) AS relevance,
    ROUND(answer_completeness, 3) AS completeness,
    CASE
        WHEN hallucination_score IS NULL OR citation_accuracy IS NULL OR
             answer_relevance IS NULL OR answer_completeness IS NULL
        THEN 'INCOMPLETE'
        ELSE 'SUCCESS'
    END AS eval_status
FROM combined_eval_results;

-- ================================================================================
-- STEP 8: Insert into EVAL_RESULTS table
-- ================================================================================

INSERT INTO EVAL_RESULTS (
    eval_id,
    interaction_id,
    eval_run_id,
    answer_relevance,
    answer_completeness,
    hallucination_score,
    citation_accuracy,
    correct_clause_ref,
    cited_clauses,
    expected_clause,
    technical_accuracy,
    judge_model,
    judge_prompt_tokens,
    judge_response_tokens,
    judge_reasoning,
    eval_method,
    eval_version,
    metadata,
    evaluated_at
)
SELECT
    UUID_STRING() AS eval_id,
    interaction_id,
    NULL AS eval_run_id,
    answer_relevance,
    answer_completeness,
    hallucination_score,
    citation_accuracy,
    NULL AS correct_clause_ref,
    NULL AS cited_clauses,
    NULL AS expected_clause,
    NULL AS technical_accuracy,
    'llama3.1-70b' AS judge_model,
    NULL AS judge_prompt_tokens,
    NULL AS judge_response_tokens,
    'Hallucination: ' || hallucination_details || ' | ' ||
    'Citations: ' || citation_details || ' | ' ||
    'Relevance: ' || relevance_details || ' | ' ||
    'Completeness: ' || completeness_details AS judge_reasoning,
    'snowflake_cortex' AS eval_method,
    '2.0-cortex' AS eval_version,
    OBJECT_CONSTRUCT(
        'hallucination_details', hallucination_details,
        'citation_details', citation_details,
        'relevance_details', relevance_details,
        'completeness_details', completeness_details,
        'unsupported_claims', unsupported_claims
    ) AS metadata,
    CURRENT_TIMESTAMP() AS evaluated_at
FROM combined_eval_results
WHERE hallucination_score IS NOT NULL  -- Only insert successful evaluations
  AND citation_accuracy IS NOT NULL
  AND answer_relevance IS NOT NULL
  AND answer_completeness IS NOT NULL;

-- ================================================================================
-- STEP 9: View inserted results
-- ================================================================================

SELECT
    interaction_id,
    ROUND(hallucination_score, 3) AS hallucination,
    ROUND(citation_accuracy, 3) AS citation,
    ROUND(answer_relevance, 3) AS relevance,
    ROUND(answer_completeness, 3) AS completeness,
    eval_method,
    eval_version,
    evaluated_at
FROM EVAL_RESULTS
WHERE eval_method = 'snowflake_cortex'
ORDER BY evaluated_at DESC
LIMIT 10;

-- ================================================================================
-- CLEANUP: Drop temporary tables
-- ================================================================================

DROP TABLE IF EXISTS interactions_to_evaluate;
DROP TABLE IF EXISTS context_prepared;
DROP TABLE IF EXISTS eval_prompt_1;
DROP TABLE IF EXISTS eval_results_1;
DROP TABLE IF EXISTS eval_prompt_2;
DROP TABLE IF EXISTS eval_results_2;
DROP TABLE IF EXISTS combined_eval_results;

-- ================================================================================
-- MONITORING QUERIES
-- ================================================================================

-- Check evaluation coverage
SELECT
    COUNT(DISTINCT i.interaction_id) AS total_interactions,
    COUNT(DISTINCT e.eval_id) AS evaluated_interactions,
    ROUND(COUNT(DISTINCT e.eval_id) * 100.0 / NULLIF(COUNT(DISTINCT i.interaction_id), 0), 2) AS coverage_pct
FROM RAG_INTERACTION i
LEFT JOIN EVAL_RESULTS e ON i.interaction_id = e.interaction_id
WHERE i.status = 'success'
  AND i.ts >= DATEADD(day, -7, CURRENT_TIMESTAMP());

-- Check evaluation quality
SELECT
    eval_method,
    COUNT(*) AS eval_count,
    ROUND(AVG(hallucination_score), 3) AS avg_hallucination,
    ROUND(AVG(citation_accuracy), 3) AS avg_citation,
    ROUND(AVG(answer_relevance), 3) AS avg_relevance,
    ROUND(AVG(answer_completeness), 3) AS avg_completeness
FROM EVAL_RESULTS
WHERE evaluated_at >= DATEADD(day, -7, CURRENT_TIMESTAMP())
GROUP BY eval_method
ORDER BY eval_count DESC;

-- ================================================================================
-- NOTES
-- ================================================================================

/*

AVAILABLE CORTEX MODELS:
- llama3.1-70b (recommended - good balance of quality and speed)
- llama3.1-405b (highest quality, slower, more expensive)
- mistral-large2 (good alternative)
- mixtral-8x7b (faster, lower cost)

BATCH PROCESSING:
Adjust the LIMIT in Step 1 to process more interactions:
    LIMIT 10   -- Small batch for testing
    LIMIT 100  -- Medium batch
    LIMIT 1000 -- Large batch

SCHEDULING WITH SNOWFLAKE TASKS:

CREATE OR REPLACE TASK eval_new_interactions_hourly
WAREHOUSE = ELECTRICAL_RAG_WH
SCHEDULE = 'USING CRON 0 * * * * UTC'  -- Every hour
AS
[Paste entire script here, wrapped in BEGIN...END]

-- Start the task
ALTER TASK eval_new_interactions_hourly RESUME;

COST OPTIMIZATION:
- Use smaller batches during testing
- Use llama3.1-70b (not 405b) for production
- Schedule during off-peak hours
- Monitor warehouse usage

TROUBLESHOOTING:
- If JSON parsing fails, check the LLM response format
- If no results, check that interactions_to_evaluate has data
- If context_text is NULL, check metadata structure
- Use the verification SELECT statements after each step

*/
