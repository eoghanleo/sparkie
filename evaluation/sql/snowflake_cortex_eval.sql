-- ================================================================================
-- Sparkie RAG Evaluation using Snowflake Cortex Complete
-- ================================================================================
--
-- This script evaluates RAG interactions stored in Snowflake using Cortex Complete
-- instead of external API calls (Groq). All data stays within Snowflake.
--
-- TWO-PROMPT EVALUATION SYSTEM:
-- 1. Context-Heavy Evaluation: Hallucination + Citation Accuracy
-- 2. Question-Answer Evaluation: Relevance + Completeness + Technical Accuracy
--
-- USAGE:
-- 1. Copy and paste into a Snowflake SQL Worksheet
-- 2. Update the database/schema names if needed
-- 3. Run the entire script or specific CTEs
-- ================================================================================

USE DATABASE SPARKIE_V2_DB;
USE SCHEMA ELECTRICAL_STANDARDS;
USE WAREHOUSE ELECTRICAL_RAG_WH;

-- ================================================================================
-- STEP 1: Get interactions that need evaluation
-- ================================================================================

WITH interactions_to_eval AS (
    SELECT
        i.interaction_id,
        i.user_query AS question,
        i.answer_text AS answer,
        i.metadata AS interaction_metadata,
        -- Parse retrieved context from metadata
        i.metadata:chunks_metadata AS chunks_metadata,
        i.metadata:visual_metadata AS visual_metadata
    FROM RAG_INTERACTION i
    LEFT JOIN EVAL_RESULTS e ON i.interaction_id = e.interaction_id
    WHERE e.eval_id IS NULL  -- Only evaluate interactions not yet evaluated
      AND i.status = 'success'
      AND i.answer_text IS NOT NULL
    LIMIT 10  -- Process in batches for testing
),

-- ================================================================================
-- STEP 2: Build context string from chunks and visual content
-- ================================================================================

context_builder AS (
    SELECT
        interaction_id,
        question,
        answer,
        -- Build full context text from chunks
        ARRAY_TO_STRING(
            ARRAY_AGG(
                '[' || ROW_NUMBER() OVER (PARTITION BY interaction_id ORDER BY chunk.value:similarity::FLOAT DESC) || '] ' ||
                '(Page ' || COALESCE(chunk.value:page_number::STRING, 'unknown') || ', ' ||
                COALESCE(chunk.value:content_type::STRING, 'text') || '):\n' ||
                COALESCE(chunk.value:text_content::STRING, chunk.value:chunk_preview::STRING, '')
            ) WITHIN GROUP (ORDER BY chunk.value:similarity::FLOAT DESC),
            '\n\n'
        ) AS context_text,
        -- Extract cited clauses from answer
        REGEXP_SUBSTR_ALL(answer, '(?:Clause|Section)\\s+([\\d.]+)', 1, 1, 'i', 1) AS cited_clauses,
        -- Count visual content
        ARRAY_SIZE(visual_metadata) AS visual_count
    FROM interactions_to_eval,
    LATERAL FLATTEN(input => chunks_metadata) chunk
    GROUP BY interaction_id, question, answer, visual_metadata
),

-- ================================================================================
-- PROMPT 1: Context-Heavy Evaluation (Hallucination + Citation)
-- ================================================================================

context_evaluation AS (
    SELECT
        interaction_id,
        question,
        answer,
        context_text,
        cited_clauses,
        -- Build prompt for Cortex Complete
        'You are an expert evaluator for a RAG system that provides AS3000 electrical standards information.

Your task is to evaluate TWO dimensions of the generated answer:
1. HALLUCINATION DETECTION - Are all claims supported by the retrieved context?
2. CITATION ACCURACY - Are AS3000 clause references correct?

═══════════════════════════════════════════════════════════════

QUESTION (from user):
' || question || '

ANSWER (generated by RAG system):
' || answer || '

RETRIEVED CONTEXT (from AS3000 standards):
' || context_text || '

CITED CLAUSES (extracted from answer): ' || ARRAY_TO_STRING(cited_clauses, ', ') || '

═══════════════════════════════════════════════════════════════

TASK 1 - HALLUCINATION DETECTION:

1. Extract all factual claims from the ANSWER (measurements, requirements, clause references, safety rules, technical specifications)
2. For each claim, verify if it appears in or is directly inferable from the RETRIEVED CONTEXT
3. List any claims that are NOT supported by the context (potential hallucinations)
4. Provide a hallucination score from 0.0 to 1.0 using precise decimals (e.g., 0.23, 0.56, 0.84):
   - 0.0-0.15: All claims fully supported by context
   - 0.16-0.35: Minor unsupported details
   - 0.36-0.65: Moderate hallucinations
   - 0.66-0.89: Significant fabricated information
   - 0.90-1.0: Severe/complete hallucinations

TASK 2 - CITATION ACCURACY:

1. Verify if the cited clauses are accurate based on the retrieved context
2. Assess if the citations are relevant to the question
3. Provide a citation accuracy score from 0.0 to 1.0 using precise decimals (e.g., 0.73, 0.41, 0.88):
   - 0.90-1.0: All citations accurate
   - 0.70-0.89: Citations mostly accurate
   - 0.35-0.69: Some citations accurate, some incorrect
   - 0.10-0.34: Most citations incorrect or irrelevant
   - 0.0-0.09: No citations or all wrong

═══════════════════════════════════════════════════════════════

OUTPUT FORMAT (JSON only, no other text):
{
    "hallucination_score": 0.0,
    "unsupported_claims": ["claim 1", "claim 2"],
    "hallucination_reasoning": "Detailed explanation of hallucination assessment",
    "citation_accuracy": 0.0,
    "correct_clause_ref": false,
    "citation_reasoning": "Detailed explanation of citation accuracy assessment"
}

Respond with ONLY the JSON object.' AS prompt_1,

        -- Call Cortex Complete
        SNOWFLAKE.CORTEX.COMPLETE(
            'llama3.1-70b',  -- Use Snowflake's Llama model
            prompt_1,
            {
                'temperature': 0.0,
                'max_tokens': 1400
            }
        ) AS context_eval_response
    FROM context_builder
),

-- Parse JSON response from Prompt 1
context_eval_parsed AS (
    SELECT
        interaction_id,
        question,
        answer,
        TRY_PARSE_JSON(context_eval_response) AS eval_json,
        COALESCE(eval_json:hallucination_score::FLOAT, NULL) AS hallucination_score,
        COALESCE(eval_json:unsupported_claims, []) AS unsupported_claims,
        COALESCE(eval_json:hallucination_reasoning::STRING, '') AS hallucination_details,
        COALESCE(eval_json:citation_accuracy::FLOAT, NULL) AS citation_accuracy,
        COALESCE(eval_json:correct_clause_ref::BOOLEAN, FALSE) AS correct_clause_ref,
        COALESCE(eval_json:citation_reasoning::STRING, '') AS citation_details,
        cited_clauses
    FROM context_evaluation
),

-- ================================================================================
-- PROMPT 2: Question-Answer Evaluation (Relevance + Completeness)
-- ================================================================================

qa_evaluation AS (
    SELECT
        interaction_id,
        question,
        answer,
        -- Build prompt for Cortex Complete
        'You are an expert evaluator for a RAG system that provides AS3000 electrical standards information.

Your task is to evaluate TWO dimensions of the generated answer:
1. RELEVANCE - Does the answer address the question?
2. COMPLETENESS - Are all parts of the question covered?

═══════════════════════════════════════════════════════════════

QUESTION (from user):
' || question || '

ANSWER (generated by RAG system):
' || answer || '

═══════════════════════════════════════════════════════════════

TASK 1 - RELEVANCE:

Rate the relevance of the ANSWER to the QUESTION on a scale from 0.0 to 1.0 using precise decimals (e.g., 0.73, 0.92, 0.45):
- 0.95-1.0: Directly and completely addresses the question
- 0.75-0.94: Mostly relevant with minor gaps
- 0.40-0.74: Partially relevant with significant gaps
- 0.15-0.39: Tangentially related
- 0.0-0.14: Completely irrelevant

TASK 2 - COMPLETENESS:

Rate the completeness of the ANSWER on a scale from 0.0 to 1.0 using precise decimals (e.g., 0.67, 0.88, 0.34):
- 0.92-1.0: All parts thoroughly addressed
- 0.70-0.91: Main points covered with minor details missing
- 0.35-0.69: Partially complete with significant gaps
- 0.10-0.34: Minimal coverage
- 0.0-0.09: Question not addressed

═══════════════════════════════════════════════════════════════

OUTPUT FORMAT (JSON only, no other text):
{
    "answer_relevance": 0.0,
    "relevance_reasoning": "Detailed explanation of relevance assessment",
    "answer_completeness": 0.0,
    "completeness_reasoning": "Detailed explanation of completeness assessment"
}

Respond with ONLY the JSON object.' AS prompt_2,

        -- Call Cortex Complete
        SNOWFLAKE.CORTEX.COMPLETE(
            'llama3.1-70b',
            prompt_2,
            {
                'temperature': 0.0,
                'max_tokens': 1000
            }
        ) AS qa_eval_response
    FROM context_eval_parsed
),

-- Parse JSON response from Prompt 2
qa_eval_parsed AS (
    SELECT
        interaction_id,
        TRY_PARSE_JSON(qa_eval_response) AS eval_json,
        COALESCE(eval_json:answer_relevance::FLOAT, NULL) AS answer_relevance,
        COALESCE(eval_json:relevance_reasoning::STRING, '') AS relevance_details,
        COALESCE(eval_json:answer_completeness::FLOAT, NULL) AS answer_completeness,
        COALESCE(eval_json:completeness_reasoning::STRING, '') AS completeness_details
    FROM qa_evaluation
),

-- ================================================================================
-- STEP 3: Combine results from both prompts
-- ================================================================================

combined_results AS (
    SELECT
        c.interaction_id,
        c.hallucination_score,
        c.unsupported_claims,
        c.hallucination_details,
        c.citation_accuracy,
        c.correct_clause_ref,
        c.citation_details,
        c.cited_clauses,
        q.answer_relevance,
        q.relevance_details,
        q.answer_completeness,
        q.completeness_details
    FROM context_eval_parsed c
    JOIN qa_eval_parsed q ON c.interaction_id = q.interaction_id
)

-- ================================================================================
-- STEP 4: Insert results into EVAL_RESULTS table
-- ================================================================================

INSERT INTO EVAL_RESULTS (
    eval_id,
    interaction_id,
    eval_run_id,
    answer_relevance,
    answer_completeness,
    hallucination_score,
    citation_accuracy,
    correct_clause_ref,
    cited_clauses,
    expected_clause,
    technical_accuracy,
    judge_model,
    judge_prompt_tokens,
    judge_response_tokens,
    judge_reasoning,
    eval_method,
    eval_version,
    metadata,
    evaluated_at
)
SELECT
    UUID_STRING() AS eval_id,
    interaction_id,
    NULL AS eval_run_id,  -- Set this if running as part of a batch
    answer_relevance,
    answer_completeness,
    hallucination_score,
    citation_accuracy,
    correct_clause_ref,
    cited_clauses,
    NULL AS expected_clause,  -- Not available for production queries
    NULL AS technical_accuracy,  -- Only for golden set evaluations
    'llama3.1-70b' AS judge_model,
    NULL AS judge_prompt_tokens,  -- Snowflake doesn't expose this
    NULL AS judge_response_tokens,
    -- Combine all reasoning into single field
    'Hallucination: ' || hallucination_details || ' | ' ||
    'Citations: ' || citation_details || ' | ' ||
    'Relevance: ' || relevance_details || ' | ' ||
    'Completeness: ' || completeness_details AS judge_reasoning,
    'cortex_complete' AS eval_method,
    '2.0-snowflake' AS eval_version,
    OBJECT_CONSTRUCT(
        'hallucination_details', hallucination_details,
        'citation_details', citation_details,
        'relevance_details', relevance_details,
        'completeness_details', completeness_details,
        'unsupported_claims', unsupported_claims
    ) AS metadata,
    CURRENT_TIMESTAMP() AS evaluated_at
FROM combined_results;

-- ================================================================================
-- STEP 5: View results
-- ================================================================================

SELECT
    interaction_id,
    hallucination_score,
    citation_accuracy,
    answer_relevance,
    answer_completeness,
    judge_model,
    eval_method,
    evaluated_at
FROM EVAL_RESULTS
WHERE eval_method = 'cortex_complete'
ORDER BY evaluated_at DESC
LIMIT 10;

-- ================================================================================
-- ALTERNATIVE: Run evaluation and view results without inserting
-- ================================================================================

-- Uncomment below to test evaluation without inserting into EVAL_RESULTS

/*
WITH interactions_to_eval AS (
    SELECT
        i.interaction_id,
        i.user_query AS question,
        i.answer_text AS answer,
        i.metadata:chunks_metadata AS chunks_metadata,
        i.metadata:visual_metadata AS visual_metadata
    FROM RAG_INTERACTION i
    LEFT JOIN EVAL_RESULTS e ON i.interaction_id = e.interaction_id
    WHERE e.eval_id IS NULL
      AND i.status = 'success'
      AND i.answer_text IS NOT NULL
    LIMIT 1  -- Test with just 1 interaction
),

context_builder AS (
    SELECT
        interaction_id,
        question,
        answer,
        ARRAY_TO_STRING(
            ARRAY_AGG(
                '[' || ROW_NUMBER() OVER (PARTITION BY interaction_id ORDER BY chunk.value:similarity::FLOAT DESC) || '] ' ||
                '(Page ' || COALESCE(chunk.value:page_number::STRING, 'unknown') || '):\n' ||
                COALESCE(chunk.value:text_content::STRING, chunk.value:chunk_preview::STRING, '')
            ) WITHIN GROUP (ORDER BY chunk.value:similarity::FLOAT DESC),
            '\n\n'
        ) AS context_text,
        REGEXP_SUBSTR_ALL(answer, '(?:Clause|Section)\\s+([\\d.]+)', 1, 1, 'i', 1) AS cited_clauses
    FROM interactions_to_eval,
    LATERAL FLATTEN(input => chunks_metadata) chunk
    GROUP BY interaction_id, question, answer
),

context_evaluation AS (
    SELECT
        interaction_id,
        question,
        answer,
        SNOWFLAKE.CORTEX.COMPLETE(
            'llama3.1-70b',
            'You are an expert evaluator... [FULL PROMPT 1 HERE]',
            {'temperature': 0.0, 'max_tokens': 1400}
        ) AS context_eval_response
    FROM context_builder
)

SELECT
    interaction_id,
    question,
    answer,
    context_eval_response
FROM context_evaluation;
*/

-- ================================================================================
-- NOTES AND TIPS
-- ================================================================================

/*

AVAILABLE CORTEX MODELS IN SNOWFLAKE:
- llama3.1-70b (recommended for evaluation - good quality, fast)
- llama3.1-405b (highest quality, slower, more expensive)
- mistral-large2 (alternative, good quality)
- mixtral-8x7b (faster, lower cost)

COST OPTIMIZATION:
- Use llama3.1-70b for production (good balance)
- Process in batches (LIMIT clause)
- Consider async evaluation with tasks/streams

BATCH PROCESSING:
To process all unevaluated interactions, remove or increase the LIMIT in the first CTE:
    LIMIT 10  -- Change to 100, 1000, or remove entirely

SCHEDULING:
You can create a Snowflake Task to run this automatically:

CREATE OR REPLACE TASK eval_new_interactions
WAREHOUSE = ELECTRICAL_RAG_WH
SCHEDULE = 'USING CRON 0 * * * * UTC'  -- Every hour
AS
[Paste the main INSERT statement here]

MONITORING:
Check evaluation coverage:

SELECT
    COUNT(DISTINCT i.interaction_id) AS total_interactions,
    COUNT(DISTINCT e.eval_id) AS evaluated_interactions,
    ROUND(COUNT(DISTINCT e.eval_id) * 100.0 / COUNT(DISTINCT i.interaction_id), 2) AS coverage_pct
FROM RAG_INTERACTION i
LEFT JOIN EVAL_RESULTS e ON i.interaction_id = e.interaction_id
WHERE i.status = 'success';

*/
