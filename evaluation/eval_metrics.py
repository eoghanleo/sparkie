"""
Evaluation Metrics for Sparkie RAG Application

Implements three critical evaluators:
1. Hallucination Detection - Verify all claims exist in retrieved context
2. Clause Reference Accuracy - Validate AS3000 citations
3. Answer Relevance - Does the response address the question?
"""

import re
import logging
from typing import Dict, List, Any, Optional, Tuple
import json
from groq import Groq
import os
from dotenv import load_dotenv
import snowflake.connector

load_dotenv()
logger = logging.getLogger(__name__)


class EvalMetrics:
    """
    Core evaluation metrics using LLM-as-judge methodology
    """

    def __init__(self, groq_api_key: Optional[str] = None, model: str = "meta-llama/llama-4-maverick-17b-128e-instruct", snowflake_conn=None):
        self.groq_client = Groq(api_key=groq_api_key or os.getenv('GROQ_API_KEY'))
        self.judge_model = model
        self.judge_temperature = 0.0  # Deterministic for evaluation
        self.snowflake_conn = snowflake_conn  # For generating presigned URLs

    def evaluate_interaction(
        self,
        question: str,
        answer: str,
        retrieved_context: List[Dict[str, Any]],
        expected_answer: Optional[str] = None,
        expected_clause: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Run all evaluation metrics on a single interaction

        Args:
            question: User's query
            answer: Generated response from Sparkie
            retrieved_context: List of retrieved chunks with metadata
            expected_answer: Golden set answer (optional)
            expected_clause: Expected AS3000 clause reference (optional)

        Returns:
            Dictionary with all evaluation scores and reasoning
        """
        results = {}

        # 1. Hallucination Detection
        hallucination_result = self.detect_hallucinations(answer, retrieved_context)
        results.update(hallucination_result)

        # 2. Clause Reference Accuracy
        clause_result = self.evaluate_clause_references(answer, expected_clause, retrieved_context)
        results.update(clause_result)

        # 3. Answer Relevance
        relevance_result = self.evaluate_answer_relevance(question, answer)
        results.update(relevance_result)

        # 4. Completeness (bonus metric)
        completeness_result = self.evaluate_completeness(question, answer)
        results.update(completeness_result)

        # 5. Technical Accuracy (if golden answer provided)
        if expected_answer:
            accuracy_result = self.evaluate_technical_accuracy(question, answer, expected_answer)
            results.update(accuracy_result)

        return results

    def detect_hallucinations(
        self,
        answer: str,
        retrieved_context: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Detect claims in the answer that are not supported by retrieved context
        Uses vision model when visual content is present

        Returns:
            {
                'hallucination_score': float (0-1, 0=no hallucinations, 1=severe hallucinations),
                'hallucination_details': str,
                'unsupported_claims': List[str]
            }
        """
        # Build context string and collect images
        context_text, vision_images = self._build_context_with_images(retrieved_context)

        prompt_text = f"""You are an expert evaluator checking if a RAG system's answer is fully supported by the retrieved context.

TASK: Identify any claims in the ANSWER that are NOT supported by the CONTEXT (text and/or images provided).

CONTEXT (retrieved from AS3000 electrical standards):
{context_text}

ANSWER (generated by RAG system):
{answer}

INSTRUCTIONS:
1. Extract all factual claims from the ANSWER (e.g., measurements, requirements, clause references, safety rules)
2. For each claim, verify if it appears in or is directly inferable from the CONTEXT (including text AND visual content in images)
3. IMPORTANT: If images are provided, examine tables, diagrams, and figures for specific values and requirements
4. List any claims that are NOT supported by the context (these are potential hallucinations)
5. Provide a hallucination score from 0.0 to 1.0, using precise decimal values (e.g., 0.23, 0.56, 0.84):
   - Use values near 0.0 (e.g., 0.0-0.15) when all claims fully supported by context
   - Use low values (e.g., 0.16-0.35) for minor unsupported details
   - Use medium values (e.g., 0.36-0.65) for moderate hallucinations
   - Use high values (e.g., 0.66-0.89) for significant fabricated information
   - Use values near 1.0 (e.g., 0.90-1.0) for severe/complete hallucinations

Provide a precise decimal score, not limited to increments of 0.1 or 0.3.

OUTPUT FORMAT (JSON):
{{
    "hallucination_score": 0.0,
    "unsupported_claims": ["claim 1", "claim 2"],
    "reasoning": "Explanation of your assessment"
}}

Respond with ONLY the JSON object, no other text."""

        try:
            # Build messages with vision support
            if vision_images:
                # Multimodal message with text + images
                content = [{"type": "text", "text": prompt_text}] + vision_images
                messages = [{"role": "user", "content": content}]
                logger.info(f"=== SENDING TO GROQ JUDGE (MULTIMODAL) ===")
                logger.info(f"Images in request: {len(vision_images)}")
                for i, img in enumerate(vision_images):
                    url = img.get("image_url", {}).get("url", "")
                    logger.info(f"  Image {i+1}: {url[:150]}...")
            else:
                # Text-only message
                messages = [{"role": "user", "content": prompt_text}]
                logger.info(f"=== SENDING TO GROQ JUDGE (TEXT ONLY) ===")

            response = self.groq_client.chat.completions.create(
                model=self.judge_model,
                messages=messages,
                temperature=self.judge_temperature,
                max_tokens=800
            )

            result_text = response.choices[0].message.content.strip()
            result = self._parse_json_response(result_text)

            return {
                'hallucination_score': result.get('hallucination_score', 0.5),
                'hallucination_details': result.get('reasoning', ''),
                'unsupported_claims': result.get('unsupported_claims', []),
                'judge_prompt_tokens': response.usage.prompt_tokens if response.usage else None,
                'judge_response_tokens': response.usage.completion_tokens if response.usage else None
            }

        except Exception as e:
            logger.error(f"Hallucination detection failed: {e}")
            return {
                'hallucination_score': None,
                'hallucination_details': f'Evaluation failed: {str(e)}',
                'unsupported_claims': []
            }

    def evaluate_clause_references(
        self,
        answer: str,
        expected_clause: Optional[str],
        retrieved_context: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Evaluate accuracy of AS3000 clause references in the answer

        Returns:
            {
                'citation_accuracy': float (0-1),
                'correct_clause_ref': bool,
                'cited_clauses': List[str],
                'expected_clause': str,
                'citation_details': str
            }
        """
        # Extract clause references from answer
        cited_clauses = self._extract_clause_references(answer)

        # Build context with page numbers for verification
        context_pages = [
            f"Page {chunk.get('page_number', 'unknown')}: {chunk.get('text_content', chunk.get('description', ''))[:200]}"
            for chunk in retrieved_context[:5]
        ]
        context_text = "\n\n".join(context_pages)

        prompt = f"""You are an expert in AS3000 electrical standards, evaluating the accuracy of clause references.

ANSWER (with clause references):
{answer}

RETRIEVED CONTEXT (with page numbers):
{context_text}

EXPECTED CLAUSE (from golden set): {expected_clause or 'Not provided'}

CITED CLAUSES (extracted): {', '.join(cited_clauses) if cited_clauses else 'None found'}

TASK:
1. Verify if the cited clauses are accurate based on the retrieved context
2. Check if the cited clauses match the expected clause (if provided)
3. Assess if the citations are relevant to the question
4. Provide a citation accuracy score from 0.0 to 1.0, using precise decimal values (e.g., 0.73, 0.41, 0.88):
   - Use values close to 1.0 (e.g., 0.90-1.0) when all citations accurate and match expected clause
   - Use high values (e.g., 0.70-0.89) when citations accurate but don't exactly match expected
   - Use medium values (e.g., 0.35-0.69) when some citations accurate, some incorrect
   - Use low values (e.g., 0.10-0.34) when most citations incorrect or irrelevant
   - Use values near 0.0 (e.g., 0.0-0.09) when no citations or all wrong

Provide a precise decimal score, not limited to increments of 0.1 or 0.2.

OUTPUT FORMAT (JSON):
{{
    "citation_accuracy": 0.0,
    "correct_clause_ref": false,
    "reasoning": "Explanation of your assessment"
}}

Respond with ONLY the JSON object, no other text."""

        try:
            response = self.groq_client.chat.completions.create(
                model=self.judge_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.judge_temperature,
                max_tokens=600
            )

            result_text = response.choices[0].message.content.strip()
            result = self._parse_json_response(result_text)

            return {
                'citation_accuracy': result.get('citation_accuracy', 0.5),
                'correct_clause_ref': result.get('correct_clause_ref', False),
                'cited_clauses': cited_clauses,
                'expected_clause': expected_clause,
                'citation_details': result.get('reasoning', '')
            }

        except Exception as e:
            logger.error(f"Clause reference evaluation failed: {e}")
            return {
                'citation_accuracy': None,
                'correct_clause_ref': False,
                'cited_clauses': cited_clauses,
                'expected_clause': expected_clause,
                'citation_details': f'Evaluation failed: {str(e)}'
            }

    def evaluate_answer_relevance(
        self,
        question: str,
        answer: str
    ) -> Dict[str, Any]:
        """
        Evaluate if the answer actually addresses the question

        Returns:
            {
                'answer_relevance': float (0-1),
                'relevance_details': str
            }
        """
        prompt = f"""You are evaluating if an answer is relevant to the question asked.

QUESTION:
{question}

ANSWER:
{answer}

TASK:
Rate the relevance of the ANSWER to the QUESTION on a continuous scale from 0.0 to 1.0, using precise decimal values (e.g., 0.73, 0.92, 0.45):
- Use values close to 1.0 (e.g., 0.95-1.0) when the answer directly and completely addresses the question
- Use high values (e.g., 0.75-0.94) when mostly relevant with minor gaps
- Use medium values (e.g., 0.40-0.74) when partially relevant with significant gaps
- Use low values (e.g., 0.15-0.39) when tangentially related
- Use values near 0.0 (e.g., 0.0-0.14) when completely irrelevant

Provide a precise decimal score based on your assessment, not limited to increments of 0.1.

OUTPUT FORMAT (JSON):
{{
    "answer_relevance": 0.0,
    "reasoning": "Explanation of your assessment"
}}

Respond with ONLY the JSON object, no other text."""

        try:
            response = self.groq_client.chat.completions.create(
                model=self.judge_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.judge_temperature,
                max_tokens=400
            )

            result_text = response.choices[0].message.content.strip()
            result = self._parse_json_response(result_text)

            return {
                'answer_relevance': result.get('answer_relevance', 0.5),
                'relevance_details': result.get('reasoning', '')
            }

        except Exception as e:
            logger.error(f"Answer relevance evaluation failed: {e}")
            return {
                'answer_relevance': None,
                'relevance_details': f'Evaluation failed: {str(e)}'
            }

    def evaluate_completeness(
        self,
        question: str,
        answer: str
    ) -> Dict[str, Any]:
        """
        Evaluate if the answer is complete (addresses all parts of the question)

        Returns:
            {
                'answer_completeness': float (0-1),
                'completeness_details': str
            }
        """
        prompt = f"""You are evaluating the completeness of an answer.

QUESTION:
{question}

ANSWER:
{answer}

TASK:
Rate the completeness of the ANSWER on a continuous scale from 0.0 to 1.0, using precise decimal values (e.g., 0.67, 0.88, 0.34):
- Use values close to 1.0 (e.g., 0.92-1.0) when all parts thoroughly addressed
- Use high values (e.g., 0.70-0.91) when main points covered with minor details missing
- Use medium values (e.g., 0.35-0.69) when partially complete with significant gaps
- Use low values (e.g., 0.10-0.34) when minimal coverage
- Use values near 0.0 (e.g., 0.0-0.09) when question not addressed

Provide a precise decimal score, not limited to increments of 0.1 or 0.2.

OUTPUT FORMAT (JSON):
{{
    "answer_completeness": 0.0,
    "reasoning": "Explanation of your assessment"
}}

Respond with ONLY the JSON object, no other text."""

        try:
            response = self.groq_client.chat.completions.create(
                model=self.judge_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.judge_temperature,
                max_tokens=400
            )

            result_text = response.choices[0].message.content.strip()
            result = self._parse_json_response(result_text)

            return {
                'answer_completeness': result.get('answer_completeness', 0.5),
                'completeness_details': result.get('reasoning', '')
            }

        except Exception as e:
            logger.error(f"Completeness evaluation failed: {e}")
            return {
                'answer_completeness': None,
                'completeness_details': f'Evaluation failed: {str(e)}'
            }

    def evaluate_technical_accuracy(
        self,
        question: str,
        answer: str,
        expected_answer: str
    ) -> Dict[str, Any]:
        """
        Evaluate technical accuracy by comparing to golden set answer

        Returns:
            {
                'technical_accuracy': float (0-1),
                'accuracy_details': str
            }
        """
        prompt = f"""You are an expert in AS3000 electrical standards, comparing two answers for technical accuracy.

QUESTION:
{question}

EXPECTED ANSWER (golden set):
{expected_answer}

ACTUAL ANSWER (generated):
{answer}

TASK:
Compare the ACTUAL ANSWER to the EXPECTED ANSWER and rate technical accuracy from 0.0 to 1.0, using precise decimal values (e.g., 0.76, 0.42, 0.91):
- Use values close to 1.0 (e.g., 0.93-1.0) when technically equivalent with same facts
- Use high values (e.g., 0.75-0.92) when mostly accurate with minor phrasing differences
- Use medium values (e.g., 0.40-0.74) when partially accurate with some technical errors
- Use low values (e.g., 0.15-0.39) when significant technical errors present
- Use values near 0.0 (e.g., 0.0-0.14) when completely wrong or contradicts expected

Provide a precise decimal score, not limited to increments of 0.1 or 0.2.

OUTPUT FORMAT (JSON):
{{
    "technical_accuracy": 0.0,
    "reasoning": "Explanation of your assessment"
}}

Respond with ONLY the JSON object, no other text."""

        try:
            response = self.groq_client.chat.completions.create(
                model=self.judge_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.judge_temperature,
                max_tokens=500
            )

            result_text = response.choices[0].message.content.strip()
            result = self._parse_json_response(result_text)

            return {
                'technical_accuracy': result.get('technical_accuracy', 0.5),
                'accuracy_details': result.get('reasoning', '')
            }

        except Exception as e:
            logger.error(f"Technical accuracy evaluation failed: {e}")
            return {
                'technical_accuracy': None,
                'accuracy_details': f'Evaluation failed: {str(e)}'
            }

    # Helper methods

    def _get_presigned_url_from_snowflake(self, content_id: str) -> Optional[str]:
        """Generate fresh presigned URL from Snowflake internal stage"""
        if not self.snowflake_conn or not content_id:
            return None

        try:
            # Extract IMAGE_ID from content_id (e.g., VISUAL_sec3_visual_025_p206 -> sec3_visual_025_p206)
            if content_id.startswith('VISUAL_'):
                image_id = content_id.replace('VISUAL_', '')
            else:
                return None

            cursor = self.snowflake_conn.cursor()
            # Get FILE_PATH from VISUAL_CONTENT_RICH and generate fresh 7-day URL
            query = f"""
                SELECT GET_PRESIGNED_URL(
                    '@AS_STANDARDS.PUBLIC_IMAGES',
                    REPLACE(FILE_PATH, '@AS_STANDARDS.PUBLIC_IMAGES/', ''),
                    604800
                )
                FROM ELECTRICAL_STANDARDS_DB.AS_STANDARDS.VISUAL_CONTENT_RICH
                WHERE IMAGE_ID = '{image_id}'
            """
            cursor.execute(query)
            result = cursor.fetchone()
            cursor.close()

            if result and result[0]:
                logger.info(f"Successfully regenerated presigned URL for {content_id}")
                return result[0]
        except Exception as e:
            logger.error(f"Failed to generate presigned URL for {content_id}: {e}")

        return None

    def _build_context_with_images(self, retrieved_context: List[Dict[str, Any]]) -> Tuple[str, List[Dict[str, Any]]]:
        """
        Build context string and collect image URLs for vision model

        Returns:
            Tuple of (context_text, vision_images_list)
        """
        context_parts = []
        vision_images = []

        logger.info(f"=== JUDGE CONTEXT BUILDING DEBUG ===")
        logger.info(f"Total retrieved_context items: {len(retrieved_context)}")

        for idx, chunk in enumerate(retrieved_context):  # NO LIMIT - see everything generation saw
            content_type = chunk.get('content_type', 'unknown')
            page = chunk.get('page_number', 'unknown')
            text_content = chunk.get('text_content', '')
            description = chunk.get('description', '')
            image_url = chunk.get('image_url', '')
            content_id = chunk.get('content_id', '')

            # Visual content - use stored URL or regenerate if needed
            if content_type == 'visual':
                # First, try to use the stored image URL from metadata (preferred)
                url_to_use = image_url if image_url else None

                # DEBUG: Log what we're receiving
                logger.info(f"VISUAL CHUNK #{idx+1}: content_id={content_id}, has_image_url={bool(image_url)}")
                if image_url:
                    logger.info(f"  URL preview: {image_url[:100]}...")

                # If no stored URL, try to regenerate from Snowflake
                if not url_to_use and content_id:
                    url_to_use = self._get_presigned_url_from_snowflake(content_id)
                    logger.info(f"Regenerated presigned URL for {content_id}")

                if url_to_use:
                    context_parts.append(
                        f"[{idx+1}] (Page {page}, VISUAL CONTENT - see image {len(vision_images) + 1}):\n"
                        f"Description: {description[:200]}"
                    )
                    # Groq vision model limit: 5 images max
                    if len(vision_images) < 5:
                        vision_images.append({
                            "type": "image_url",
                            "image_url": {"url": url_to_use}
                        })
                        logger.info(f"✓ Added visual content to judge context: {content_id}")
                    else:
                        logger.warning(f"⚠ Skipping visual {content_id} - Groq 5-image limit reached")
                else:
                    # Fallback to description only if no URL available
                    context_parts.append(
                        f"[{idx+1}] (Page {page}, VISUAL CONTENT - image unavailable):\n"
                        f"Description: {description[:500]}"
                    )
                    logger.warning(f"✗ No URL available for visual content: {content_id}")
            else:
                # Regular text content
                text = text_content or description
                context_parts.append(f"[{idx+1}] (Page {page}, {content_type}):\n{text[:500]}")

        context_text = "\n\n".join(context_parts)

        logger.info(f"=== JUDGE CONTEXT SUMMARY ===")
        logger.info(f"Total context chunks: {len(context_parts)}")
        logger.info(f"Visual images prepared: {len(vision_images)}")
        logger.info(f"Context text length: {len(context_text)} chars")

        return context_text, vision_images

    def _extract_clause_references(self, text: str) -> List[str]:
        """
        Extract AS3000 clause references from text

        Patterns:
        - AS3000 Clause 3.7.2.1
        - Clause 3.7.2
        - Section 3.7
        - AS/NZS 3000:2018 Clause 5.5.2
        """
        patterns = [
            r'AS/?NZS?\s*3000(?::\d{4})?\s+(?:Clause|Section)\s+([\d.]+)',
            r'(?:Clause|Section)\s+([\d.]+)',
            r'AS3000\s+([\d.]+)'
        ]

        clauses = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            clauses.extend(matches)

        # Deduplicate and sort
        return sorted(set(clauses))

    def _parse_json_response(self, text: str) -> Dict[str, Any]:
        """Parse JSON from LLM response, handling markdown code blocks"""
        # Remove markdown code blocks if present
        text = re.sub(r'^```json\s*', '', text, flags=re.MULTILINE)
        text = re.sub(r'^```\s*', '', text, flags=re.MULTILINE)
        text = text.strip()

        try:
            return json.loads(text)
        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse JSON response: {e}\nText: {text}")
            return {}


# Convenience function for single interaction evaluation
def evaluate_rag_response(
    question: str,
    answer: str,
    retrieved_context: List[Dict[str, Any]],
    expected_answer: Optional[str] = None,
    expected_clause: Optional[str] = None
) -> Dict[str, Any]:
    """
    Evaluate a single RAG interaction with all metrics

    Returns complete evaluation results
    """
    evaluator = EvalMetrics()
    return evaluator.evaluate_interaction(
        question=question,
        answer=answer,
        retrieved_context=retrieved_context,
        expected_answer=expected_answer,
        expected_clause=expected_clause
    )
